---
title: "L'analyse de données single-cell RNA-seq, Partie 1"
excerpt: ""
collection: guidelines
date: 2023-09-21
---


<i>Author: Marin Truchi</i>


## Analyse primaire, des fichiers FASTQ aux matrices de comptes

Comme tout séquençage [Illumina paired end](https://www.illumina.com/content/dam/illumina-marketing/documents/products/illumina_sequencing_introduction.pdf "An Introduction to Next-Generation Sequencing Technology"), celui des librairies single-cell RNA-seq génère 3 types de reads répartis dans autant de fichiers FASTQ. Ces 3 types de reads sont reliés par un identifiant unique qui forme leur en-tête, et sont chacun associés à un score de qualité. Dans le cas où plusieurs librairies de séquençage ont été multiplexées sur une même flow cell, les reads « I1 » contiennent l’index inséré lors de la préparation pour distinguer chaque librairie. L’étape de démultiplexage attribue chaque triplé de reads aux différents échantillons, ou à un échantillon unique dans le cas où une seule librairie a été séquencée. Les reads « R2 » correspondent aux 91 paires de bases séquencées à partir de l’extrémité 3’ de la molécule d’ADNc du transcrit capturé. Cette séquence est alignée sur le génome ou le transcriptome de référence de l’espèce pour assigner le read à un gène. Les reads « R1 » encodent 16 paires de bases correspondant au barcode cellulaire et 12 paires de bases correspondant au barcode moléculaire. Ce barcode moléculaire, appelé UMI (Unique Molecular Identifier), permet d’identifier de manière unique chaque ARN capturé avant l’étape d’amplification par PCR. La quantification finale consiste à compter le nombre de UMI unique assignés à un même gène. De ce fait, les biais de quantification induits par des niveaux d’amplification PCR inégaux sont évités. Le barcode cellulaire relie quant à lui chaque read à une droplet et par extension à la cellule encapsulée. Les matrices de comptes des molécules d’ARN de chaque gène dans chaque cellule sont ainsi obtenues à partir des lectures combinées des reads R1 et R2 partageant le même identifiant.


<figure>
  <img src="/images/cellranger.PNG" alt="Analyse primaire"/>
  <figcaption>Analyse primaire

**A**, Reads obtenus après séquençage illumina.
**B**, Traitement des reads et comptage des UMI.
**C**, Catégories de reads alignés sur le transcriptome.
**D**, Classification des barcodes en fonction de la quantité d’UMI.
Adapté du [manuel d’utilisation](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/algorithms/overview "Cell ranger manual") de cellranger fourni par 10X Genomics.</figcaption>
</figure>


Cette quantification des transcrits est réalisée par différents pipelines combinant plusieurs outils d’analyses, notamment le pipeline cellranger développé par la société 10X Genomics. Cellranger se base sur le logiciel d’alignement STAR pour aligner les reads « R2 » sur le génome de référence. Les reads non alignés ou dont la qualité d’alignement n’est pas satisfaisante sont éliminés. De plus, les annotations transcriptomiques sont utilisées pour définir la part de reads exoniques, c’est-à-dire qui s’alignent sur des exons sur au moins 50% de leur longueur, ainsi que les reads introniques et intergéniques. Si les reads alignés sur des régions intergéniques sont éliminés, une option permet d’inclure les reads alignés avec confiance sur des régions introniques. Considérés comme ceux alignés avec confiance sur le transcriptome, les reads restant sont quantifiés par
comptage des UMI.

L’objectif final est de se baser sur cette quantification des UMI afin d’identifier les barcodes qui correspondent à des droplets dans lesquels une cellule a été capturée et éliminer ceux qui correspondent à des droplets vides susceptibles de bruiter le signal. La matrice de compte finale récapitule les profils d’expression des gènes détectés dans les barcodes filtrés et constitue le point de départ de l’analyse secondaire.

Chaque analyse primaire produit un résumé des différentes métriques calculés au cours du processus. Parmi ces métriques, on retrouve l’estimation du nombre de barcodes correspondant à des cellules capturées, leur nombre moyen de reads et d’UMI, les pourcentages des reads séquencés issus de ces cellules ou issus de droplets vides, ou encore la répartition des reads entre les régions exoniques, introniques et intergéniques. Une autre métrique intéressante est la saturation de séquençage, qui mesure le pourcentage de transcrits uniques détectés par rapport à leur nombre total estimé pour la librairie. Cette estimation dépend à la fois de la profondeur de séquençage initialement allouée, et de la complexité du transcriptome des populations cellulaires étudiées. Une saturation faible signifie qu’un nouveau séquençage de la librairie peut être bénéfique pour augmenter le nombre d’UMI détectés. Ces métriques sont donc particulièrement informatives pour réaliser une première évaluation de la qualité des données générées.


#
## Analyse secondaire, des matrices de comptes au clustering

### Caractéristiques des matrices de comptes
Les matrices de comptes récapitulant l’expression des genes pour chaque barcode assigné à une cellule présentent des caractéristiques particulières qui déterminent le processus d’analyse. Tout d’abord, il s’agit de données en haute dimensions : pour plusieurs milliers de barcodes, environ 30000 genes sont quantifiés. Cependant, l’écrasante majorité des éléments de la matrice est constituée de comptes nuls ou quasi nuls, du fait de plusieurs facteurs.

Ce phénomène s’explique tout d’abord par les quantités infimes de molécules détectables à l’échelle de la cellule unique et par une profondeur de séquençage faible, du fait de la répartition d’un nombre défini de reads entre plusieurs milliers de cellules. Il s’explique également par la variabilité biologique du tissu étudié, qui implique des mécanismes de régulations spécifiques à chaque population qui n’expriment pas toutes les mêmes gènes. De plus, les processus menant à l’expression des gènes suivent des dynamiques discontinues et désynchronisées entre les cellules, dont le séquençage ne saisit qu’un état instantané. Au-delà de cette variabilité biologique s’ajoute la variabilité technique induite par la nature stochastique de la capture des ARN. Seulement une fraction de la totalité des transcrits cellulaires est en effet capturée (environ 10% avec le protocole d’isolement des cellules par droplets de 10X Genomics), et toutes les molécules n’ont pas la même probabilité de l’être. Cette probabilité dépend notamment de la stabilité des transcrits, de leur localisation subcellulaire et de leur séquence.

 En raison de ces nombreux facteurs, les données scRNA-seq sont caractérisées par une importante variabilité intercellulaire. Le principal enjeu de l’analyse secondaire est de traiter les données de sorte à extraire le signal biologique d’intérêt en réduisant au maximum les autres sources de variabilité non désirées ou non informatives.

### Etape 1 - Contrôle qualité
La première étape de l’analyse des matrices de comptes est d’effectuer un contrôle qualité de sorte à filtrer les barcodes et les gènes se rapportant à du bruit. Les gènes exprimés dans seulement quelques barcodes n’étant pas pertinents pour caractériser l’hétérogénéité biologique de l’échantillon, leur élimination est recommandée pour réduire les dimensions de la matrice et accélérer le temps de calcul des opérations.

Le contrôle des barcodes est davantage crucial pour réduire la part de bruit susceptible de masquer la variabilité biologique. Si les barcodes correspondant à des droplets vides ont été préalablement éliminés, les barcodes restants sont susceptibles de représenter des cellules dégradées ou des doublets, qui risquent de biaiser l’analyse. L’identification des cellules non viables se base sur 3 variables calculées pour chaque barcode : leur nombre total d’UMI, leur nombre total de gènes uniques détectés et leur fraction de comptes correspondant à des transcrits mitochondriaux. Une fraction anormalement élevée de genes mitochondriaux est interprétée comme le résultat d’une dégradation de l’intégrité de la membrane cellulaire, aboutissant à la perte d’ARN cytoplasmique tandis que les transcrits mitochondriaux sont conservés. Cette dégradation potentielle se matérialise aussi généralement par une diminution du nombre de comptes d’UMI et du nombre de gènes détectés.

Les cellules considérées comme de mauvaise qualité sont éliminées en appliquant des seuils arbitrairement déterminés par rapport à la distribution de ces 3 métriques sur l’ensemble du jeu de données. Une certaine souplesse est cependant préconisée quant à la fixation de ces seuils, de sorte à éviter l’exclusion de types ou d’états cellulaires caractérisées par un faible contenu transcriptomique ou un niveau plus élevé de transcrits mitochondriaux. Plus généralement, le contrôle qualité s’apparente à un procédé itératif dont les paramètres initiaux doivent être ajustés compte tenu des effets observés sur le regroupement final des cellules. Typiquement, des cellules regroupées en fonction de leur contenu en UMI, de leur part élevée de transcrits mitochondriaux, ou marquées par une signature de stress, plutôt que par l’expression de transcrits biologiquement pertinents, sont à éliminer a posteriori.

<figure>
  <img src="/images/QC.PNG" alt="Contrôle qualité"/>
  <figcaption>Variables de contrôle qualité</figcaption>
</figure>

Un autre biais à éliminer dans cette étape de l’analyse est la présence de barcodes correspondant à des doublets, des droplets dans lesquelles au moins deux cellules ont été encapsulées, et dont la fréquence augmente proportionnellement avec le nombre de cellules profilées (entre 5 et 12% d’un jeu de données de 5000 à 10000 cellules). La surreprésentation de doublets rassemblant 2 types cellulaires différents peut amener à fausser l’interprétation en les considérant par exemple comme un état intermédiaire ou comme un type cellulaire rare. Si les doublets présentent généralement un contenu en nombre d’UMI ou en nombre de gènes détectés plus élevés que les barcodes associées à des cellules uniques, leur identification se base sur des méthodes plus complexes. Ces approches consistent à créer des doublets artificiels en agrégeant les profils de paires de cellules puis à les intégrer au jeu de données, avant de réaliser un clustering. La probabilité pour chaque barcode de correspondre à un doublet est ensuite calculée en mesurant la proximité entre son profil d’expression et celui des doublets artificiels dans l’espace de dimension réduite.


### Etape 2 - Normalisation
L’élimination des barcodes présentant des valeurs extrêmes, notamment en ce qui concerne le nombre total d’UMI quantifiés, est une première étape nécessaire pour débruiter le signal. Dans les données correspondant à des barcodes considérés comme des cellules en bon état, la part d’hétérogénéité biologique reste cependant largement mêlée à d’autres facteurs de variabilités, biaisant ainsi l’analyse. La normalisation de ces données a pour but de traiter deux de ces facteurs confondant inhérents à l’approche, en effectuant 2 opérations simultanées.

La première consiste à corriger les fluctuations techniques du nombre total de molécules quantifiées dans chaque cellule. Cette variabilité de profondeur est notamment due aux différences d’efficacités de capture, de rétrotranscription et de séquençage de leurs transcrits. Sans correction ce signal indésirable est susceptible de représenter une part prédominante de la variance utilisée pour la réduction de dimensions, l’étape clé sur laquelle se base le clustering et la représentation des données. Les cellules auront alors tendance à se regrouper selon leur profondeur moyenne plutôt que par similarité d’expression de gènes spécifiques de leurs fonctions ou d’une condition biologique. La comparaison de l’abondance des transcrits entre cellules risque également de produire des faux positifs dont l’expression différentielle est liée à l’écart de profondeur.

La seconde opération consiste à corriger l’hétéroscédasticité des données, c’est-à-dire les différences de variances entre les comptes de chaque gène, notamment entre les gènes plus exprimés qui présentent une dispersion plus importante que ceux les moins exprimés. Cette corrélation entre la moyenne et la variance implique que les opérations statistiques basées sur la détection de gènes variables favoriseront les gènes les plus abondants, quelle que soit la pertinence biologique de leur variabilité. Pour limiter ce biais, la normalisation des données doit donc s’accompagner d’une étape de stabilisation des variances. Enfin, la transformation induite par la normalisation doit être monotone, c’est-à-dire qu’elle doit conserver les rangs d’importance attribuées à chaque variable afin de rester fidèle à la réalité biologique. Dans ce contexte, différentes méthodes de normalisation ont été proposées, sans que leurs performances ne soient encore à ce jour considérées comme satisfaisantes.

La méthode la plus utilisée par la communauté est conceptuellement identique à la normalisation CPM utilisée en "bulk" RNA-seq. Elle consiste à mettre à l’échelle chaque compte en le divisant par la somme totale des UMI comptabilisés dans la cellule et en le multipliant par un facteur d’ajustement unique, par exemple la médiane des comptes ou la puissance de 10 du même ordre de grandeur (en général 10 000). Pour la stabilisation de la variance, les données normalisées sont log-transformées en ajoutant un pseudo-compte de 1 pour éviter le calcul de logarithme de valeurs nulles. La transformation en log réduit en effet la corrélation entre la profondeur et la variance. La réduction de la dispersion des données ainsi induite permet également d’approximer une distribution normale des comptes, une hypothèse sous-jacente pour la plupart des outils d’analyse statistique. Elle présente enfin l’avantage de traduire les différences d’expression en « fold change », une mesure standard et facilement interprétable.

<figure>
  <img src="/images/Normalisation.PNG" alt="Effet de la normalisation sur la relation entre la moyenne et la variance des comptes"/>
  <figcaption>Effet de la normalisation sur la relation entre la moyenne et la variance des comptes</figcaption>
</figure>


### Etape 3 - Réduction de dimension
La normalisation des données réduit la part de variance d’origine technique. Le signal biologique d'intérêt reste cependant noyé par la trop grande dimensionnalité des données. Par exemple, si tous les gènes sont pris en comptes pour établir des distances entre les profils d’expression, alors toutes les cellules auront tendance à se retrouver équidistantes les unes des autres. De plus, parmi les milliers de gènes quantifiés, seulement une fraction d’entre eux contribuent à la caractérisation des types et des états cellulaires. Une sélection des variables informatives et la réduction de la dimensionnalité des données sont donc nécessaires pour réduire le ratio signal-bruit et améliorer l’interprétabilité des données.

La sélection des variables informatives est généralement basée sur la relation établie entre la variance et la moyenne de tous les comptes normalisés. Les gènes dont la variance observée est supérieure à celle attendue sont considérés comme hautement variables et donc comme les plus susceptibles de décrire l’hétérogénéité du jeu de données. Les gènes hautement variables sélectionnés sont ensuite employés pour la réduction de dimension.

L’approche de réduction de dimension majoritairement employée est l’analyse en composante principale (ACP), décrite par Karl Pearson en 1901. L’ACP consiste à créer des variables artificielles, appelées composantes principales, qui correspondent à des axes de projection sur lesquelles la variance des données en haute dimension est maximisée. La somme des distances au carré qui relient les points des données projetées sur l’axe et l’origine de référence est appelée valeur propre. La première composante principale est celle qui détient la plus grande valeur propre et les composantes suivantes sont hiérarchisées par ordre décroissant de valeur propre. Chaque composante est une combinaison linéaire des contributions de chaque gène à l’axe de projection qu’elle représente. Autrement dit, il s’agit de vecteurs associant un score à chaque gène, appelés vecteurs propre. L’ACP présente plusieurs avantages. Premièrement, le calcul des vecteurs propres demande peu de ressources de calcul et s’applique facilement à de gros jeux de données. De plus, la nature des vecteurs propres facilite l’interprétation des composantes. Les composantes corrélées à des marqueurs de types et d’états cellulaires sont ainsi directement identifiables. De manière analogue, des composantes peuvent être corrélées à des gènes associés à des différences de profondeur ou autre source de variabilité indésirable et indiquer ainsi des lacunes dans le contrôle qualité ou la normalisation. Les composantes principales calculées sont les variables sur lesquelles sont basées le clustering et la visualisation des données. Le choix du nombre de composantes à inclure dans chacune de ces opérations est empirique, un nombre trop faible pouvant induire une perte d’information tandis qu’un nombre trop élevé est susceptible d’apporter du bruit technique. Ce choix s’appuie premièrement sur la fraction de la variance totale du dataset expliquée par chaque composante, calculée à partir des valeurs propres de chaque composante. Lorsque l’inclusion d’une nouvelle composante n’augmente plus significativement la part de variance totale, alors sa contribution à l’hétérogénéité du jeu de données est probablement négligeable. De plus, la sélection des composantes peut reposer sur l’inspection supervisée des gènes qui y contribuent le plus, de sorte à conserver celles qui comportent des signatures biologiquement pertinentes ou éliminer celles qui s’apparentent à du bruit.

### Etape 4 - Clustering
L’ensemble des procédures de traitement, de normalisation et de réductions de dimensions des données décrites ci-dessus ont pour finalité d’isoler le signal biologique permettant de regrouper les cellules selon leurs profils transcriptomiques, qui témoignent de leurs fonctions et de leurs réponses à leur environnement. Cette étape de clustering se base sur les distances entre les profils d’expression dans l’espace de dimensions réduites pour identifier des communautés de cellules similaires.

La méthode de clustering de référence est celle des « k plus proches voisins » (k Nearest Neighbors ou kNN). Elle consiste tout d’abord à calculer la matrice des distances euclidiennes séparant chaque cellule, en se basant sur leur vecteur de coordonnées dans l’espace réduit en n dimensions, où n est le nombre de composantes principales sélectionnées. Les distances calculées sont reportées dans une représentation en graphe ou chaque noeud est une cellule. Seules les arêtes du graphe reliant chaque noeud à ces k plus proches voisins sont conservées. Un algorithme de partitionnement du graphe, tel que celui dit de Louvain, est ensuite employé pour isoler les communautés de cellules les plus proches. Lorsque la taille des communautés sous-jacentes est hétérogène, l’algorithme peut échouer dans la détection des communautés les plus petites. Pour y remédier, un paramètre de résolution peut être ajusté de sorte à favoriser la formation de petites communautés, correspondant par exemple à des types cellulaires rares. Une haute résolution aura cependant tendance à éclater les grandes communautés pourtant homogènes en plusieurs sous-ensembles dont les caractéristiques qui les discriminent sont généralement peu pertinentes d’un point de vue biologique.

### Etape 5 - Annotation
La pertinence des communautés identifiées par le clustering est évaluée lors de leur annotation, qui se base sur la détection de gènes dits marqueurs pour leur associer une fonction, un état, une réponse, ou une localisation. Ces marqueurs sont identifiés par des tests de différence de moyenne d’expression entre les cellules de chaque cluster et le reste du jeu de données, ou par l’inspection manuelle de gènes connus.

Idéalement, un cluster biologiquement pertinent présente une signature de plusieurs dizaines de marqueurs dont l’expression est forte, spécifique, et dont les fonctions sont connues dans la littérature, de sorte à identifier la population cellulaire associée. Dans la pratique, certains clusters montrent une expression de gènes élevée mais qui n’est pas particulièrement spécifique. C’est notamment le cas d’états cellulaires transitoires de processus de différentiation, qui présentent généralement des signatures similaires aux populations dont ils sont issus tout en ayant perdu certains marqueurs de ces populations spécialisées. D’autres clusters sont caractérisés par l’expression spécifique de seulement quelques marqueurs, ce qui réduit les chances pour que cette population détienne une identité biologique propre. Inversement, un cluster peut être marqués par une signature distincte de taille importante, mais qui correspond à une variabilité non désirée. Par exemple, la signature de stress induite par la dissociation enzymatique à 37° est marquée par l’expression massive de gènes codants pour des protéines de choc thermique et pour des facteurs de transcriptions de la famille des gènes précoces immédiats, tels que JUN, JUNB, c-FOS, FOSB, ATF3, ou EGR140. Un contrôle qualité ou une normalisation inopérants mènent à la création de clusters dont la signature reflète des biais techniques, comme un taux de gènes mitochondriaux ou par une profondeur moyenne aberrants. Enfin, dans le cas où différents échantillons sont intégrés, des clusters spécifiques d’une partie des échantillons peuvent se démarquer du fait de gènes dont la détection varie selon les conditions de séquençage. De même, l’expression de certains gènes est déterminée par les caractéristiques génétiques de l’individu, comme par exemple les gènes dont le locus se situent sur le chromosome Y et donc spécifiquement détectés chez les mâles.

Une difficulté majeure de l’annotation est donc de lier les signatures des clusters à des labels faisant biologiquement sens. L’information spatiale étant perdue lors de la dissociation du tissu, l’annotation repose exclusivement sur les connaissances empiriques établies pour les gènes des signatures identifiées. Ces connaissances sont directement accessibles quand l’analyste ou son entourage est expert du domaine. Cependant, il advient fréquemment que certaines signatures ne soient pas immédiatement reconnues, ce qui nécessite une étape de recherche bibliographique qui peut s’avérer complexe et chronophage. En effet, malgré l’enrichissement constant des bases de données répertoriant les marqueurs des populations cellulaires, toutes ne sont pas recensées dans la littérature, ou ces marqueurs ne sont pas forcément applicables aux données scRNA-seq. Par exemple, l’usage de gènes identifiés comme marqueurs via leur protéine, tels que les marqueurs de surface, peut s’avèrer infructueuse du fait de la corrélation limitée entre les deux mesures. De plus, si la plupart des gènes différentiellement exprimés dans une population sont des gènes codants annotés au niveau fonctionnel, ces descriptions ne renseignent par forcément sur la fonction de la cellule. Ces annotations sont en effet issues d’expériences réalisées dans un contexte et un modèle précis, qui correspondent rarement à celui de l’expérience menant au séquençage.

Néanmoins, grâce à l’augmentation exponentielle du nombre de jeux de données single-cell RNA-seq manuellement annotés et publiés, de nombreuses méthodes d’annotations automatiques tirant profit de ces ressources sont aujourd’hui disponibles et représentent une alternative intéressante pour faciliter le processus.


### Etape 6 - Visualisation des données
Pour faciliter l’exploration et l’interprétation des données, les cellules sont systématiquement représentées dans un espace en 2 dimensions où chaque cluster est identifiable, de sorte à visualiser l’expression des gènes dans les différents clusters. Dans cette optique, la représentation des cellules projetées dans l’espace des deux premières composantes principales de l’ACP n’est pas idéale. En effet, si elles expliquent généralement la majorité de la variabilité du jeu de données, les gènes qui contribuent à ces deux axes ne suffisent pas pour expliquer toutes les différences entre chaque communauté de cellules. Les cellules des clusters les plus similaires auront tendance à se superposer, empêchant ainsi de visualiser en détails les différents clusters et l’expression particulières des transcrits. Pour y remédier, des techniques de réductions de dimensions non linéaires ont été proposées, dont les plus populaires sont le t-SNE et le UMAP. Usuellement, ces deux approches sont réalisées à partir des résultats de l’ACP afin d’accélérer le processus et d’être plus cohérent avec le clustering kNN définissant les communautés de cellules, également basé sur l’ACP.

Brièvement, l’objectif de ces approches est de créer un espace de projection des cellules en 2 dimensions préservant les distances entre les cellules dans l’espace en haute dimension (en l'occurrence celui des n composantes principales choisies), de sorte à ce que les cellules similaires apparaissent proches et les cellules dissemblables apparaissent séparées. Pour cela, un score de similarité est calculé entre chaque cellule à partir de leur distance dans l'espace en haute dimension, puis dans l'espace en 2 dimensions, en fixant une loi de probabilité particulière pour chaque score. L’algorithme corrige ensuite la position de chaque cellule dans l’espace en 2 dimensions en minimisant la divergence entre les distributions des scores de similarités dans l’espace en haute dimension et
dans l’espace en 2 dimensions.

Du fait des choix mathématiques implémentées, le t-SNE présente 3 limites majeures :
* Le t-SNE pénalise fortement la situation où les courtes distances dans
l’espace en hautes dimensions ne sont pas respectées dans l’espace en 2 dimensions. Inversement, les situations où les longues distances dans l’espace en hautes dimensions ne sont pas respectées dans l’espace en 2 dimensions ne sont pas pénalisées. De ce fait, la représentation en t-SNE a tendance à préserver les distances entre les cellules les plus proches (distances intra-cluster) mais néglige la conservation des distances entre cellules éloignées (distances inter-clusters). Par conséquent, la structure globale des données est ignorée au profit des structures plus locales.
* Avec le t-SNE, l’initialisation de la projection des cellules dans l’espace en 2 dimensions ainsi que la fonction de minimisation sont basée sur un modèle stochastique, c’est-à-dire que l’algorithme peut converger sur différentes solutions. Deux t-SNE calculés sur les mêmes données peuvent ainsi produire des représentations différentes.
* La minimisation étant calculée par itération cellule par cellule, l’algorithme du t-SNE est particulièrement lent à converger vers une solution optimale, ce qui le rend rédhibitoire pour une application sur des jeux de données de plusieurs dizaines de milliers de cellules

Comparé au t-SNE, l'algorithme UMAP pénalise les situations où les longues distances dans l’espace en hautes dimensions ne sont pas respectées dans l’espace en 2 dimensions. Par conséquent, la représentation en UMAP préserve théoriquement mieux la structure globale des données d’origine. De plus, contrairement au t-SNE où la balance entre la conservation des structures locales et globales est empiriquement ajustable, cette balance est définie pour la UMAP par un nombre de plus proches voisins, préférentiellement le même que celui utilisé pour le clustering. Globalement, l’algorithme UMAP est plus déterministe dans le sens où la représentation finale est moins sensible à la variation des paramètres. Enfin, le principal avantage de la UMAP vis-à-vis du t-SNE est sa rapidité d’exécution, qui en fait la méthode préférentielle pour la visualisation des jeux de données importants.

<figure>
  <img src="/images/Projections.PNG" alt="Propriétés des algorithmes de visualisation 2D des données : tSNE et UMAP"/>
  <figcaption>Propriétés des algorithmes de visualisation 2D des données : tSNE et UMAP</figcaption>
</figure>


Les méthodes t-SNE et UMAP sont des algorithmes mathématiquement complexes qui ont été rapidement adoptés en bioinformatique pour répondre au problème de visualisation posée par les données en hautes dimensions. Dans l’analyse de données scRNA-seq, elles font partie des représentations inéluctables pour décrire l’hétérogénéité d’un jeu de données ou l’expression des gènes, tant au niveau pratique qu’esthétique. Cependant, leur seule utilisation graphique s’est retrouvée largement extrapolée, sans tenir compte des limites d’interprétation imposées par leurs fondements mathématiques. La visualisation en 2 dimensions produite par le t-SNE ou la UMAP est en effet systématiquement utilisée comme support à l’exploration des données. Par exemple, elles servent à repérer des communautés de cellules isolées des autres, émettre des hypothèses sur des trajectoires de différenciation ou encore évaluer l’efficacité de la normalisation et de l’intégration de jeux de données, en se basant uniquement sur une interprétation visuelle. Or, il est démontré que si ces représentations reflètent en partie la topologie des données initiales, la réduction drastique des dimensions opérée s’accompagne d’un phénomène de distorsion massif des distances entre les profils transcriptomiques cellulaires120. Cette déformation des relations spatiales s’applique autant aux distances entre cellules voisines qu’aux distances entre clusters différents. Théoriquement plus respectueuse des structures globales, la représentation en UMAP n’est, dans les faits, pas plus performante. Dans ce contexte, les formes des clusters et les distances les séparant les uns des autres représentées en t-SNE ou en UMAP ne doivent en aucun cas faire l’objet de surinterprétations.


### Etape optionnelle - Intégration de données issues de séquençages indépendants

Du fait des aléas techniques induits lors du prélèvement, de la préparation et du séquençage des ARN de chaque échantillon, les données issues de différentes expériences présentent généralement une forte variabilité. Cette variabilité est encore accrue lorsque ces expériences ont été réalisées avec différents protocoles et avec différents expérimentateurs. Elle se traduit à la fois au niveau de la profondeur moyenne obtenue mais également au niveau de caractéristiques transcriptomiques particulières, le tout étant résumé sous le terme « d’effet batch ». Sans traitement particulier et malgré la normalisation de la profondeur, ces sources de variations sont susceptibles de biaiser l’analyse intégrée des échantillons et de masquer le signal biologique d’intérêt. Typiquement, le risque principal est de détecter des communautés de cellules spécifiques d’un échantillon, alors qu’elles correspondent à un type ou un état cellulaire partagé avec d’autres. Inversement, une sur-correction des données risque d’effacer le signal biologique, par exemple en agglomérant des cellules hétérogènes au sein d’un même cluster. Trouver l’équilibre de correction peut s’avérer complexe lorsque les échantillons à intégrer sont issus de différentes conditions ou diagnostics qui induisent des variations transcriptionnelles spécifiques. Une intégration réussie garantie ainsi la correction de l’effet batch tout en conservant la variabilité biologique du jeu de données et en mobilisant le moins de ressources de calcul. Les performances de dizaines d’outils d’intégration qui ont été proposés dans la littérature ont été comparés selon ces mêmes critères. Si ces performances apparaissent globalement équivalentes entre les approches, elles sont surtout dépendantes de la nature des données à intégrer, notamment le nombre total de cellules considérées, la complexité du jeu de données, le degré de variabilité biologique et technique entre les échantillons, ou encore le type de traitement appliqué aux données.



## Erreurs à éviter lors de la première partie de l'analyse
* Négliger les métriques issues de l'alignement et de la quantification
* Choisir des filtres de contrôle qualité trop stringents
* Réaliser une sur-correction des données lors de la normalisation/transformation, ou de l'intégration
* Annoter des clusters biologiquement non-pertinents
* Interpréter les distances entres cellules à partir des projections tSNE et UMAP


### Article suivant

[L'analyse de données single-cell RNA-seq, Partie 2](/guidelines/231101_MT_scRNAseq_analysis_2 "L'analyse de données single-cell RNA-seq, Partie 2").


## Bibliographie

Luecken, M. D. & Theis, F. J. Current best practices in single-cell RNA-seq analysis: a tutorial. Mol. Syst. Biol. 15, e8746 (2019)

Zheng, G. X. Y. et al. Massively parallel digital transcriptional profiling of single cells. Nat. Commun. 8, 14049 (2017).

Denisenko, E. et al. Systematic assessment of tissue dissociation and storage biases in single-cell and single-nucleus RNA-seq workflows. Genome Biol. 21, 130 (2020).

Ilicic, T. et al. Classification of low quality cells from single-cell RNA-seq data. Genome Biol. 17, 29 (2016).

Booeshaghi, S., Hallgrímsdóttir, I., Gálvez-Merchán, Á. & Pachter, L. Depth normalization for single-cell genomics count dat. Preprint at https://doi.org/10.1101/2022.05.06.490859 (2022).

Chari, T. & Pachter, L. The Specious Art of Single-Cell Genomics. 2021.08.25.457696 Preprint at https://doi.org/10.1101/2021.08.25.457696 (2022).

Luecken, M.D., Büttner, M., Chaichoompu, K. et al. Benchmarking atlas-level data integration in single-cell genomics. Nat Methods 19, 41–50 (2022).
